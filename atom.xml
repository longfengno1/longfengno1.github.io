<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Xiao&#39;s Note</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://longfengno1.github.io/"/>
  <updated>2016-04-11T01:36:21.403Z</updated>
  <id>https://longfengno1.github.io/</id>
  
  <author>
    <name>Xiao</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>数据挖掘算法的R语言实现</title>
    <link href="https://longfengno1.github.io/2016/04/11/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E7%AE%97%E6%B3%95%E7%9A%84R%E8%AF%AD%E8%A8%80%E5%AE%9E%E7%8E%B0/"/>
    <id>https://longfengno1.github.io/2016/04/11/数据挖掘算法的R语言实现/</id>
    <published>2016-04-11T01:24:27.000Z</published>
    <updated>2016-04-11T01:36:21.403Z</updated>
    
    <content type="html">&lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;学习数据挖掘已经有一段时间了，相关的文章和书也看了一些，感觉学习这个的关键还是离不开其中形形色色的算法。作为一个初学者，我们也不奢求创新改进个算法。先从基础做起，学会各个基础算法的思想与实现。学习算法的过程是十分枯燥的，但是如果学习的过程能够实践，例如使用R语言实践一下，将一堆头痛眼花的数据转化成一张炫酷的图，这无疑是十分有成就感的。所以，我就最近学习的资料，整理了一些算法与R语言的实现方法分享一下。由于篇幅的问题，后面提到的函数我都没有详细介绍了，想了解的可以使用&lt;code&gt;&amp;gt;?函数名&lt;/code&gt;或&lt;code&gt;&amp;gt;??函数名&lt;/code&gt;查看。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;分类&quot;&gt;&lt;a href=&quot;#分类&quot; class=&quot;headerlink&quot; title=&quot;分类&quot;&gt;&lt;/a&gt;分类&lt;/h1&gt;&lt;h2 id=&quot;1、KNN算法&quot;&gt;&lt;a href=&quot;#1、KNN算法&quot; class=&quot;headerlink&quot; title=&quot;1、KNN算法&quot;&gt;&lt;/a&gt;1、KNN算法&lt;/h2&gt;&lt;p&gt;K——最临近方法（k Nearest Neighbors，简称KNN）是实际运用中经常被采用的一种基于距离的分类算法。&lt;/p&gt;
&lt;p&gt;基本思想：&lt;br&gt;假定每个类包含多个训练数据，且每个训练数据都有一个唯一的类别标记，计算每个训练数据到待分类元组的距离，取和待分类元组距离最近的k个训练数据，k个数据中哪个类别的训练数据占多数，则待分类元组就属于哪个类别。&lt;/p&gt;
&lt;p&gt;主要函数：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;knn()&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;加载R中的class库：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&gt;library(class)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;实例：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&gt;data(iris3)&lt;br&gt;#选取前30个数据作为训练数据&lt;br&gt;&gt;train&amp;lt;-rbind(iris[1:30,,1],iris[1:30,,2],iris[1:30,,3])&lt;br&gt;#剩下的作为测试数据&lt;br&gt;&gt;test&amp;lt;-rbind(iris[31:50,,1],iris[31:50,,2],iris[31:50,,3])&lt;br&gt;&gt;c1&amp;lt;-factor(c(rep(“s”,30),rep(“c”,30),rep(“v”,30)))&lt;br&gt;#进行KNN算法分类&lt;br&gt;&gt;knn(train,test,c1,k=3,prob=TRUE)&lt;br&gt;&gt;attributes(.Last.value)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;2、决策树算法（C4-5）&quot;&gt;&lt;a href=&quot;#2、决策树算法（C4-5）&quot; class=&quot;headerlink&quot; title=&quot;2、决策树算法（C4,5）&quot;&gt;&lt;/a&gt;2、决策树算法（C4,5）&lt;/h2&gt;&lt;p&gt;主要函数：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;J48()&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;准备工作：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&gt;install.packages(‘rJava’)&lt;br&gt;&gt;install.packages(‘party’)&lt;br&gt;&gt;install.packages(‘RWeka’)&lt;br&gt;&gt;install.packages(‘partykit’)&lt;br&gt;&gt;library(RWeka)&lt;br&gt;&gt;library(party)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;实例：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&gt;oldpar=par(mar=c(3,3,1.5,1),mgp=c(1.5,0.5,0),cex=0.3)&lt;br&gt;&gt;data(iris)&lt;br&gt;&gt;m1&amp;lt;-J48(Species~.,data=iris)&lt;br&gt;&gt;m1&lt;br&gt;&gt;table(iris$Species,predict(m1))&lt;br&gt;&gt;write_to_dot(m1)&lt;br&gt;&gt;if(require(“party”,quietly=TRUE)) plot(m1)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;生成树如下：&lt;br&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/1863202-0377acaefc8ac97d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;C4,5生成的决策树&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;3、CART算法&quot;&gt;&lt;a href=&quot;#3、CART算法&quot; class=&quot;headerlink&quot; title=&quot;3、CART算法&quot;&gt;&lt;/a&gt;3、CART算法&lt;/h2&gt;&lt;p&gt;CART(Classification and Regression Tree，分类与回归树)。&lt;br&gt;主要函数：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;tree()&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;准备工作：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&gt;install.packages(‘tree’)&lt;br&gt;&gt;library(tree)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;实例：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;#设置窗口参数&lt;br&gt;&gt;oldpar=par(mar=c(3,3,1.5,1),mgp=c(1.5,0.5,0),cex=0.7)&lt;br&gt;&gt;data(iris)&lt;br&gt;#对品种进行CART分类&lt;br&gt;&gt;ir.tr=tree(Species~.,iris)&lt;br&gt;&gt;summary(ir.tr)&lt;br&gt;#画决策树图&lt;br&gt;&gt;plot(ir.tr):text(ir.tr)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;生成树如下：&lt;br&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/1863202-060c899e0b5b0022.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;CART生成的决策树&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;4、BP神经网络算法&quot;&gt;&lt;a href=&quot;#4、BP神经网络算法&quot; class=&quot;headerlink&quot; title=&quot;4、BP神经网络算法&quot;&gt;&lt;/a&gt;4、BP神经网络算法&lt;/h2&gt;&lt;p&gt;主要函数：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;nnet()&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;准备工作：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&gt;install.packages(‘nnet’)&lt;br&gt;&gt;library(nnet)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;实例：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&gt;data(iris3)&lt;br&gt;&gt;ir&amp;lt;-rbind(iris3[,,1],iris3[,,2],iris3[,,3])&lt;br&gt;&gt;targets&amp;lt;-class.ind(c(rep(“s”,50),rep(“c”,50),rep(“v”,50)))&lt;br&gt;#抽取25个样本&lt;br&gt;&gt;samp&amp;lt;-c(sample(1:50,25),sample(51:100,25),sample(101:150,25))&lt;br&gt;&gt;ir1&amp;lt;-nnet(ir[samp,],targets[samp,],size=2,rang=0.1,decay=5e-4,maxit=200)&lt;br&gt;&gt;test.c1&amp;lt;-function(true,pred){&lt;br&gt;true&amp;lt;-max.col(true)&lt;br&gt;cres&amp;lt;-max.col(pred)&lt;br&gt;table(true,cres)&lt;br&gt;}&lt;br&gt;#对样本以外的数据的测试&lt;br&gt;&gt;test.c1(targets[-samp,],predict(ir1,ir[-samp,]))&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;聚类&quot;&gt;&lt;a href=&quot;#聚类&quot; class=&quot;headerlink&quot; title=&quot;聚类&quot;&gt;&lt;/a&gt;聚类&lt;/h1&gt;&lt;h2 id=&quot;1、K-means算法&quot;&gt;&lt;a href=&quot;#1、K-means算法&quot; class=&quot;headerlink&quot; title=&quot;1、K-means算法&quot;&gt;&lt;/a&gt;1、K-means算法&lt;/h2&gt;&lt;p&gt;K-means算法是典型的基于距离的聚类算法，采用距离作为相似性的评价指标，即认为两个对象的距离越近，其相似度就越大。&lt;/p&gt;
&lt;p&gt;主要函数：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;kmeans()&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;实例：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;#随机生成样本数据&lt;br&gt;&gt;x&amp;lt;-rbind(matrix(rnorm(10000,sd=0.3),ncol=10),matrix(rnorm(10000,mean=1,sd=0.3),ncol=10))&lt;br&gt;&gt;colnames(x)&amp;lt;-c(“x1”,”x2”,”x3”,”x4”,”x5”,”x6”,”x7”,”x8”,”x9”,”x10”)&lt;br&gt;#调用K-means算法&lt;br&gt;&gt;c1&amp;lt;-Kmeans(x,2)&lt;br&gt;&gt;pch1=rep(“1”,1000)&lt;br&gt;&gt;pch2=rep(“2”,1000)&lt;br&gt;&gt;plot(x,col=c1$cluster,pch=c(pch1,pch2))&lt;br&gt;&gt;points(c1$centers,col=3,pch=”*”,cex=3)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;2、PAM算法&quot;&gt;&lt;a href=&quot;#2、PAM算法&quot; class=&quot;headerlink&quot; title=&quot;2、PAM算法&quot;&gt;&lt;/a&gt;2、PAM算法&lt;/h2&gt;&lt;p&gt;PAM(Partitioning around Medoid，围绕中心点的划分)是最早提出的k-medoids算法之一。它试图对n个对象给出k个划分。最初随机选择k个中心点后，该算法反复地试图找出更好的中心点。&lt;/p&gt;
&lt;p&gt;主要函数：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;pam()&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;准备工作：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&gt;library(cluster)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;实例：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&gt;pamx=pam(x,2)&lt;br&gt;&gt;summary(pamx)&lt;br&gt;&gt;plot(pamx,main=”pam效果图”)   #数据集同上&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;3、Clara算法&quot;&gt;&lt;a href=&quot;#3、Clara算法&quot; class=&quot;headerlink&quot; title=&quot;3、Clara算法&quot;&gt;&lt;/a&gt;3、Clara算法&lt;/h2&gt;&lt;p&gt;主要思想：不考虑整个数据集合，选择实际数据的一小部分作为数据的样本，然后用PAM方法从样本中选择中心点。如果样本是以随机形式选取的，它应当足以代表原来的数据集合。从中选出的代表对象（中心点）很可能与从整个数据集合中选出的非常近似Clara抽取数据集合的多个样本，对每个样本应用PAM算法，返回最好的聚类结果作为输出。&lt;/p&gt;
&lt;p&gt;主要函数：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;clara()&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;准备工作：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&gt;library(cluster)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;实例：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&gt;clarax=clara(x,2)&lt;br&gt;&gt;clarax&lt;br&gt;&gt;clarax$clusinfo&lt;br&gt;&gt;plot(clarax,main=”clara图”)  #数据集同上&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;层次聚类&quot;&gt;&lt;a href=&quot;#层次聚类&quot; class=&quot;headerlink&quot; title=&quot;层次聚类&quot;&gt;&lt;/a&gt;层次聚类&lt;/h1&gt;&lt;h2 id=&quot;1、AGNES算法与DIANA算法&quot;&gt;&lt;a href=&quot;#1、AGNES算法与DIANA算法&quot; class=&quot;headerlink&quot; title=&quot;1、AGNES算法与DIANA算法&quot;&gt;&lt;/a&gt;1、AGNES算法与DIANA算法&lt;/h2&gt;&lt;p&gt;AGNES(Agglomerative Nesting)算法是凝聚的层次聚类方法。最初将每个对象作为一个簇，然后这些簇根据某些准则一步步地合并，直到所有的对象最终合并到一个簇中或某个终结条件被满足。&lt;br&gt;DIANA(Divisive ANAlysis)算法是分裂的层次聚类方法。采用自顶向下的策略，它首先将所有对象置于一个簇中，然后逐渐细分为越来越小的簇，直到每个对象自成一簇或某个终结条件被满足。&lt;/p&gt;
&lt;p&gt;主要函数：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;agnes()、diana()&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;准备工作：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&gt;library(cluster)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;实例：&lt;br&gt;AGNES和DIANA算法的比较&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;#将图形显示区划为两部分&lt;br&gt;&gt;par(mfrow=c(1,2))&lt;br&gt;&gt;data(flower)&lt;br&gt;&gt;dai.f=daisy(flower,type=list(asymm=3,ordratio=7))&lt;br&gt;&gt;agn.f=agnes(dai.f,method=”ward”)&lt;br&gt;&gt;plot(agn.f,which.plot=2,cex=0.7,yaxt=”n”,main=”agnes算法的聚类图”)&lt;br&gt;&gt;dia.f=diana(dai.f)  #注意这里dia.f与dai.f不同&lt;br&gt;&gt;plot(dia.f,which.plot=2,main=”diana算法的聚类图”)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;结果图如下：&lt;br&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/1863202-2128ea9d6676999a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;agnes与diana算法的比较(这个图画的有点丑，大家可以自己试下。。。)&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;基于密度聚类&quot;&gt;&lt;a href=&quot;#基于密度聚类&quot; class=&quot;headerlink&quot; title=&quot;基于密度聚类&quot;&gt;&lt;/a&gt;基于密度聚类&lt;/h1&gt;&lt;p&gt;主要思想：只要临近区域的密度（对象或数据点的数目）超过某个阀值，就继续聚类。&lt;/p&gt;
&lt;p&gt;优点：可以过滤“噪声”孤立点数据，发现任意形状的簇。&lt;/p&gt;
&lt;h2 id=&quot;1、DBSCAN算法&quot;&gt;&lt;a href=&quot;#1、DBSCAN算法&quot; class=&quot;headerlink&quot; title=&quot;1、DBSCAN算法&quot;&gt;&lt;/a&gt;1、DBSCAN算法&lt;/h2&gt;&lt;p&gt;DBSCAN(Density-Based Spatial Clustering of Application with Noise)是一个有代表性的基于密度的方法，它根据一个密度阀值来控制簇的增长。&lt;/p&gt;
&lt;p&gt;主要函数：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;DBSCAN()&lt;br&gt;准备工作：&lt;br&gt;&gt;library(cluster)&lt;br&gt;实例：&lt;br&gt;&gt;dflower&amp;lt;-daisy(flower,type=list(asymm=c(“V1”,”V3”),symm=2,norminal=4,ordinal=c(5,6),ordratio=7,logratio=8))&lt;br&gt;&gt;DBF=DBSCAN(dflower,eps=0.65,MinPts=5,distances=T)&lt;br&gt;&gt;DBF&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;基于模型聚类&quot;&gt;&lt;a href=&quot;#基于模型聚类&quot; class=&quot;headerlink&quot; title=&quot;基于模型聚类&quot;&gt;&lt;/a&gt;基于模型聚类&lt;/h1&gt;&lt;h2 id=&quot;1、COBWEB算法&quot;&gt;&lt;a href=&quot;#1、COBWEB算法&quot; class=&quot;headerlink&quot; title=&quot;1、COBWEB算法&quot;&gt;&lt;/a&gt;1、COBWEB算法&lt;/h2&gt;&lt;p&gt;COBWEB是一种流行的简增量概念聚类算法。它以一个分类树的形式创建层次聚类，每个节点对应一个概念，包含该概念的一个概率描述，概述被分在该节点下的对象。&lt;/p&gt;
&lt;p&gt;主要函数：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Cobweb()&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;准备工作：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&gt;install.packages(‘RWeka’)&lt;br&gt;&gt;library(RWeka)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;实例：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&gt;com=rbind(cbind(rnorm(20,0,0.5),rnorm(20,0,0.5)),cbind(rnorm(30,5,0.5),rnorm(30,5,0.5)))&lt;br&gt;&gt;clas=factor(rep(2:1,c(20,30)))&lt;br&gt;&gt;dcom=data.frame(com,clas)&lt;br&gt;&gt;c1&amp;lt;-Cobweb(dcom)&lt;br&gt;&gt;c1&lt;br&gt;&gt;c1$class_ids&lt;br&gt;&gt;table(predict(c1),dcom$clas)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;模糊聚类&quot;&gt;&lt;a href=&quot;#模糊聚类&quot; class=&quot;headerlink&quot; title=&quot;模糊聚类&quot;&gt;&lt;/a&gt;模糊聚类&lt;/h1&gt;&lt;h2 id=&quot;1、FCM算法&quot;&gt;&lt;a href=&quot;#1、FCM算法&quot; class=&quot;headerlink&quot; title=&quot;1、FCM算法&quot;&gt;&lt;/a&gt;1、FCM算法&lt;/h2&gt;&lt;p&gt;FCM(Fuzzy C-Means)算法是一个模糊聚类算法，不同于硬划分，模糊聚类方法是一个软划分。对于模糊集来说，一个数据点都是以一定程度属于某个类，也可以同时以不周的程度属于几个类。&lt;/p&gt;
&lt;p&gt;主要函数：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;fanny()&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;准备工作：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&gt;library(cluster)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;实例：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&gt;z=rbind(cbind(rnorm(100,0,0.5),rnorm(100,0,0.5)),cbind(rnorm(150,5,0.5),rnorm(150,5,0.5),cbind(rnorm(300,3.2,0.5),rnorm(300,3.2,0.5))))&lt;br&gt;&gt;z&lt;br&gt;&gt;fannyz=fanny(z,3,metric=”SqEuclidean”)&lt;br&gt;&gt;summary(fannyz)&lt;br&gt;&gt;plot(fannyz,main=”模糊算法聚类图”)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;参考文献&lt;br&gt;&lt;em&gt;方匡南. 基于数据挖掘的分类和聚类算法研究及R语言实现[D]. 暨南大学, 2007.&lt;/em&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;学习数据挖掘已经有一段时间了，相关的文章和书也看了一些，感觉学习这个的关键还是离不开其中形形色色的算法。作为一个初学者，我们也不奢求创新改进个算法。先从基础做起，学会各个基础算法的思想与实现。学习算法的过程是十分枯燥的，但是如果学习的过程能够实践，例如使用R语言实践一下，将一堆头痛眼花的数据转化成一张炫酷的图，这无疑是十分有成就感的。所以，我就最近学习的资料，整理了一些算法与R语言的实现方法分享一下。由于篇幅的问题，后面提到的函数我都没有详细介绍了，想了解的可以使用&lt;code&gt;&amp;gt;?函数名&lt;/code&gt;或&lt;code&gt;&amp;gt;??函数名&lt;/code&gt;查看。&lt;br&gt;
    
    </summary>
    
      <category term="数据挖掘" scheme="https://longfengno1.github.io/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
    
      <category term="R语言" scheme="https://longfengno1.github.io/tags/R%E8%AF%AD%E8%A8%80/"/>
    
  </entry>
  
  <entry>
    <title>数据预处理</title>
    <link href="https://longfengno1.github.io/2016/04/05/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/"/>
    <id>https://longfengno1.github.io/2016/04/05/数据预处理/</id>
    <published>2016-04-05T08:42:44.000Z</published>
    <updated>2016-04-06T02:50:40.295Z</updated>
    
    <content type="html">&lt;p&gt;在对数据进行分类前，对数据的预处理可以提高分类预测的准确性、有效性和可伸缩性。以下是几种数据预处理：&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;br&gt;1、数据清理：为了消除和减少数据噪声和处理缺失值的数据预处理。虽然大部分的分类算法都会处理噪声和缺失值，但在进行分类对数据的清理可以减少学习时的混乱。&lt;br&gt;2、相关性分析：数据中很多属性可能与分类预测任务不相关或是冗余的。因此在分类前进行相关性分析可以删除学习过程中不相关的或是冗余的属性，提高分类预测的效率和准确率。&lt;br&gt;3、数据变换：分类前的数据变换主要有概念分层和规范化两种。概念分层就是连续值属性概化为离散的区间，压缩了原来的训练数据，学习时可以减少输入输出操作。规范化是将给定属性的所有值按比例缩放，使得它们落入较小的指定区间，比如落入【0，1】内，可以防止具有较大初始域的属性相对于具有较小初始域的属性权种过大，该方法常用于神经网络和距离度量方法。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;引自：方匡南. 基于数据挖掘的分类和聚类算法研究及R语言实现[D]. 暨南大学, 2007.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&quot;数据缺失&quot;&gt;&lt;a href=&quot;#数据缺失&quot; class=&quot;headerlink&quot; title=&quot;数据缺失&quot;&gt;&lt;/a&gt;数据缺失&lt;/h1&gt;&lt;p&gt;当处理含有缺失值（NA）的数据时，可以运用以下几种最常见的策略：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;将含有缺失值的案例剔除。&lt;/li&gt;
&lt;li&gt;根据变量之间的相关性关系填补缺失值。&lt;/li&gt;
&lt;li&gt;根据案例之间的相似性填补缺失值。&lt;/li&gt;
&lt;li&gt;使用能够处理缺失值数据的工具。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下文的例子都用到了”DMwR”包，读取数据代码如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; library(DMwR)
&amp;gt; data(algae)
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;将缺失部分剔除&quot;&gt;&lt;a href=&quot;#将缺失部分剔除&quot; class=&quot;headerlink&quot; title=&quot;将缺失部分剔除&quot;&gt;&lt;/a&gt;将缺失部分剔除&lt;/h2&gt;&lt;p&gt;将含有缺失值的案例剔除非常容易实现，尤其是当这些记录所占的比例在可用数据集中非常小的时候，这个选择就比较合理。因此，我们在选择这个方案时先检查观测值，或者至少得到这些观测值的个数。&lt;br&gt;例如：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; algae[!complete.case(algae),]
 ...
 ...
&amp;gt; nrow(algae[!complete.case(algae),])
 [1] 16
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;函数complete.case()产生一个布尔值向量，该向量的元素个数与algae数据框中的行数相同，如果数据框的相应行中不含NA值，则函数返回TRUE。如果直接删除所有含有至少一个NA的样本，我们可以输入：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; algae&amp;lt;-na.omit(algae)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然而这种办法太过极端，我们一般不采用。但是对于某些缺失值太多的样本我们可以直接剔除，因为他们几乎是无用的样本。观测方法可通过如下代码：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; apply(algae,1,function(x) sum(is.na(x)))
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然而之前我们加载的”DMwR”包中有相关处理函数，应用如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; algae&amp;lt;-algae[-manyNAs(algae),]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;函数manyNAs()的功能是找出缺失值个数大于列数20%的行，第二个参数默认值是0.2。&lt;/p&gt;
&lt;h2 id=&quot;用最高频率来填补缺失值&quot;&gt;&lt;a href=&quot;#用最高频率来填补缺失值&quot; class=&quot;headerlink&quot; title=&quot;用最高频率来填补缺失值&quot;&gt;&lt;/a&gt;用最高频率来填补缺失值&lt;/h2&gt;&lt;p&gt;填补含有缺失值记录的另一个方法是尝试找到这些缺失值最可能的值。这里也有多种策略可以选择，不同策略对逼近程度和算法复杂度的权衡不同。&lt;br&gt;填补缺失数据最简便和快捷的方法是使用一些代表中心趋势的值。代表中心趋势的值反映了变量分布的最常见值，因此中心趋势值是最自然的选择。然而，中心趋势值也有很多种，如平均值、中位数、众数等。如何选择还要由变量的分布决定。对于接近正态分布来说，所有的观测值都较好地聚集在平均值周围，平均数就是最佳选择。然而对于偏态分布，平均值就不适用。另一方面，离群值（极值）的存在会扭曲平均值（这些可以通过箱式图观测到）。下面我们列举几个填补例子：&lt;br&gt;用平均值填补：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; algae[48,&amp;quot;mxPH&amp;quot;]&amp;lt;-mean(algae$mxPH,na.rm=T)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;用中位数填补：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; algae[is.na(algae$Chla),&amp;quot;Chla&amp;quot;]&amp;lt;-median(algae$Chla,na.rm=T)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;其中，na.rm是使计算时忽略缺失数据。当然，我们例子中用到的包下也提供了一个函数centralImputation()可以用数据的中心趋势值来填补缺失值。对数值型变量使用中位数，对名义变量使用众数。应用如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; data(algae)
&amp;gt; algae&amp;lt;-algae[-manyNAs(algae),]
&amp;gt; algae&amp;lt;-centralImputation(algae）
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;上述方法虽然快捷方便，但是它可能导致较大的数据偏差，影响后期的数据分析工作。&lt;/p&gt;
&lt;h2 id=&quot;通过变量的相关关系来填补缺失值&quot;&gt;&lt;a href=&quot;#通过变量的相关关系来填补缺失值&quot; class=&quot;headerlink&quot; title=&quot;通过变量的相关关系来填补缺失值&quot;&gt;&lt;/a&gt;通过变量的相关关系来填补缺失值&lt;/h2&gt;&lt;p&gt;另一种获得较少偏差填补缺失值的方法是探寻变量之间的相关关系。我们可以通过以下命令：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; symnum(cor(algae[,4:18],use=&amp;quot;complete.obs&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;函数cor()的功能是产生变量之间的相关值矩阵，设定参数use=”complete.obs”可以使R在计算相关值时忽略含有NA的纪录。而函数symnum()是用来改善结果的输出形式的。&lt;br&gt;在找到相关性较高的两个变量后，我们开始寻找他们之间的线性相关关系，如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; data(algae)
&amp;gt; algae&amp;lt;-algae[-manyNAs(algae),]
&amp;gt; lm(PO4~oPO4,data=algae)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后，我们通过线性关系计算缺失值的填补值。&lt;br&gt;如果PO4中存在多个缺失值，我们也可以通过构造一个函数来完成，如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; data(algae)
&amp;gt; algae&amp;lt;-algae[-manyNAs(algae),]
&amp;gt; fillPO4&amp;lt;-function(oP){
  if(is.na(oP))
     return(NA)
  else return(42.897+1.293*oP)
 }
&amp;gt; algae[is.na(algae$PO4),&amp;quot;PO4&amp;quot;]&amp;lt;-sapply(algae[is.na(algae$PO4),&amp;quot;oPO4&amp;quot;],fillPO4)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;函数sapply()的第一个参数是一个向量，第二个参数是一个函数。作用是将函数结果应用到第一个参数中向量的每一个元素。&lt;br&gt;在上面的例子中由于我们使用的是相关值，所以只用到了数值变量而排除了名义变量与缺失值之间的关系。针对这个，我们可以通过直方图等形式进行观测（这里不多赘述，感兴趣的可以查阅原书）。&lt;/p&gt;
&lt;h2 id=&quot;通过探索案例之间的相似性来填补缺失值&quot;&gt;&lt;a href=&quot;#通过探索案例之间的相似性来填补缺失值&quot; class=&quot;headerlink&quot; title=&quot;通过探索案例之间的相似性来填补缺失值&quot;&gt;&lt;/a&gt;通过探索案例之间的相似性来填补缺失值&lt;/h2&gt;&lt;p&gt;除了变量之间的相关性外，多个样本（行）之间的相似性也可以用来填补缺失值。度量相似性的指标有很多，常用的是欧式距离，这个距离可以非正式的定义为任何两个案例之的观测值之差的平方和。书中提供的包的函数knnImputation()可以实现上述操作，这个函数用一个欧式距离的变种来找到任何个案最近的k个邻居。使用方法如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; algae&amp;lt;-knnImputation(algae,k=10)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;用中位数来填补：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; algae&amp;lt;-knnImputation(algae,k=10,meth=&amp;quot;median&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&quot;小结&quot;&gt;&lt;a href=&quot;#小结&quot; class=&quot;headerlink&quot; title=&quot;小结&quot;&gt;&lt;/a&gt;小结&lt;/h1&gt;&lt;p&gt;综合上述几个方法，各有优缺点，具体如何选择还应该根据分析领域的知识来确定。此外，根据个案之间的相似性来填补缺失值也有不合理处，例如可能存在不相关的变量扭曲相似性。对于这些大数据集问题，可以通过随机抽取样本的方法来计算它们之间的相似性。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;引自：《数据挖掘与R语言》（葡）Luis Torgo著&lt;/em&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;在对数据进行分类前，对数据的预处理可以提高分类预测的准确性、有效性和可伸缩性。以下是几种数据预处理：&lt;br&gt;
    
    </summary>
    
      <category term="数据挖掘" scheme="https://longfengno1.github.io/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
    
      <category term="R语言" scheme="https://longfengno1.github.io/tags/R%E8%AF%AD%E8%A8%80/"/>
    
  </entry>
  
  <entry>
    <title>Hexo+Github搭建博客中的一些小问题</title>
    <link href="https://longfengno1.github.io/2016/03/23/Hexo+Github%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B0%8F%E9%97%AE%E9%A2%98/"/>
    <id>https://longfengno1.github.io/2016/03/23/Hexo+Github搭建博客中的一些小问题/</id>
    <published>2016-03-23T07:34:58.000Z</published>
    <updated>2016-03-25T01:29:10.613Z</updated>
    
    <content type="html">&lt;p&gt;经历了两天多的辛苦，终于完成了自己博客的搭建。我用的是Hexo+Github搭建的静态博客，在参考了很多大神的&lt;a href=&quot;http://ibruce.info/2013/11/22/hexo-your-blog/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;教程&lt;/a&gt;下，最后也算是磕磕绊绊地完成了吧（当然还有很多不足的地方=。=）。&lt;br&gt;&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;&lt;br&gt;为什么突然会选择Hexo和Github来搭建自己的博客呢？一是因为markdown，这里强烈推荐下！刚接触markdown时就爱上了这门语言，写文章什么的太方便了，基本不用再用word什么的处理下了。其次，也是一个很重要的原因：Github上搭博客是免费的！作为一个程序猿，什么开源的、免费的东西当然是我最喜欢的东西。&lt;/p&gt;
&lt;p&gt;好了，废话也不多说了，开这个博客的初衷也就是希望能和大家分享一些经验和相互学习。所以，作为我的第一篇文章，就和大家分享一下我在搭建博客中遇到的一些问题。简单的就不多说了，百度一下&lt;strong&gt;Hexo+Github&lt;/strong&gt;就会出来一堆教程。而且一些&lt;a href=&quot;http://theme-next.iissnan.com/getting-started.html#comment-system-duoshuo&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;官方文档&lt;/a&gt;什么的也说的很详细（我用的是Hexo下的Next主题）。下面是我遇到的一些比较纠结的问题：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1、本地测试时，&lt;a href=&quot;http://localhost:4000&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://localhost:4000&lt;/a&gt;一直登不上。&lt;/strong&gt;&lt;br&gt;首先，对于Hexo 3.X版本，服务器被独立成了个别模块，所以你必须先安装hexo-server才能使用。在GitBash里输入：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$ npm install hexo-server –save&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;安装完成后输入以下命令：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$ hexo server&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;或&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$ hexo s&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;然后提示你你的网站在&lt;a href=&quot;http://localhost:4000&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://localhost:4000&lt;/a&gt;下启动。&lt;br&gt;我在这里的时候就一直登不上，估计是4000的端口被占用了。最后使用下面的命令修改了端口就成功，如下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$ hexo s -p 5000&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;2、在配置_config.yml后，运行hexo g出错。&lt;/strong&gt;&lt;br&gt;这里大部分的错误就由于没有在“:”后面加空格时所导致的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3、使用hexo deploy部署时老是失败&lt;/strong&gt;&lt;br&gt;这个问题是浪费我时间最多的一个问题了。每次使用hexo deploy部署时就会出现下图的情况：&lt;br&gt;&lt;img src=&quot;http://7xs74n.com1.z0.glb.clouddn.com/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;百度了很多方法，最后还在&lt;a href=&quot;https://github.com/hexojs/hexo/issues/857&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;这个&lt;/a&gt;上面找到了答案。我仔细观察了下自己文档下都没有.deploy这个文件。。所以后来重新又建了一个项目，按照官方文档一步步操作后才成功了。&lt;br&gt;以上就是我搭建过程中的一些经验了，如果感兴趣的或有问题的我们还可以一起讨论。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;经历了两天多的辛苦，终于完成了自己博客的搭建。我用的是Hexo+Github搭建的静态博客，在参考了很多大神的&lt;a href=&quot;http://ibruce.info/2013/11/22/hexo-your-blog/&quot;&gt;教程&lt;/a&gt;下，最后也算是磕磕绊绊地完成了吧（当然还有很多不足的地方=。=）。&lt;br&gt;
    
    </summary>
    
      <category term="随笔" scheme="https://longfengno1.github.io/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="Hexo,Github" scheme="https://longfengno1.github.io/tags/Hexo-Github/"/>
    
  </entry>
  
</feed>
